{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgensim_fixt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_module\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m     22\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/gensim/matutils.py:1034\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/gensim/_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "#from methods import *\n",
    "import re\n",
    "import benepar\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.draw\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw.tree import TreeView\n",
    "from nltk.stem import WordNetLemmatizer as wln\n",
    "import os\n",
    "from IPython.display import display\n",
    "# import spacy\n",
    "from  icecream import ic\n",
    "from scipy.stats import chisquare\n",
    "from nltk.corpus import wordnet as wn\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "import numpy as np\n",
    "from nltk.data import find\n",
    "from nltk.test.gensim_fixt import setup_module\n",
    "import gensim\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_enders = {\".\", '?', '!', ':', ';'}\n",
    "sentential_tags = {'S', 'SBAR', 'SINV', 'SBARQ', 'SQ'}\n",
    "negators = {'not', 'n\\'t'}\n",
    "verb_tags = {'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'VB', 'MD'}\n",
    "adverb_tags = {'RB', 'RBR', 'RBS', 'WRB'}\n",
    "adj_tags = {'JJ', 'JJR', 'JJS'}\n",
    "noun_tags = {'NN', 'NNP', 'NNS', 'NNPS'}\n",
    "tensed = {'have', 'do', 'will', 'would', 'can', 'could', 'may', 'might', 'should', 'ought', 'must', 'shall', 'be', 'dare'}\n",
    "wh_tags = {'WDT', 'WP', 'WP$', 'WRB'}\n",
    "mod_tags = {'JJ', 'JJR', 'JJS', 'DT', 'PDT', 'CD'}\n",
    "prep_tags = {'IN', 'TO'}\n",
    "\n",
    "setup_module()\n",
    "tagger = Classifier.load('sentiment')\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in an array of english words and returns true if one of a \n",
    "# set of words is present\n",
    "def english_indicator(text, options=['only', 'just']):\n",
    "  for w in text:\n",
    "    if w in options:\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "# takes in a line of english text, splits the line into paragraphs\n",
    "# returns a line splinto sentences as an array of array of strings\n",
    "# if containing all alphanumeric characters, or if a specified\n",
    "# other condition is met\n",
    "def english_processor(text, specific_processor=None):\n",
    "  res = []\n",
    "\n",
    "  #https://stackoverflow.com/questions/25735644/python-regex-for-splitting-text-into-sentences-sentence-tokenizing\n",
    "  sentences = re.split(r'(?<!\\w\\.\\w.)(?<!\\b[A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?)\\s|\\\\n', text)\n",
    "\n",
    "  sentences = [s.split(' ') for s in sentences]\n",
    "\n",
    "\n",
    "  sentences = [[w.lower() for w in s if w not in { '\\n', '<p>', '', ',', '.', '\\\"', '\\'', '?', '!', ':', ';'}] for s in sentences]\n",
    "\n",
    "  sentences = [s for s in sentences if '@' not in s]\n",
    "\n",
    "  sentences = [s for s in sentences]\n",
    "\n",
    "  for s in sentences:\n",
    "    if not s:\n",
    "      continue\n",
    "    if '@' in s:\n",
    "      continue\n",
    "    joined = ''.join(s)\n",
    "    if '#' in joined:\n",
    "      continue\n",
    "    if ':' in joined:\n",
    "      continue\n",
    "    if ';' in joined:\n",
    "      continue\n",
    "    if '-' in joined:\n",
    "      continue\n",
    "    res.append(s)\n",
    "  return res\n",
    "\n",
    "def new_processor(text, specific_processor=None):\n",
    "  res = nltk.sent_tokenize(text)\n",
    "  ic(res)\n",
    "  return res\n",
    "\n",
    "\n",
    "# takens in the path to a .txt file, a function that takes in a \n",
    "# line containing multiple sentences and returns array of array\n",
    "# of words in sentences, and a function that return true iff a \n",
    "# sentence is valid\n",
    "def valid_sentences(textpath, processor, indicator):\n",
    "  res = []\n",
    "  i = 0\n",
    "  with open(textpath, 'r') as f:\n",
    "    for line in f:\n",
    "      #print(line)\n",
    "      sentences = [s for s in processor(line) if indicator(s)]\n",
    "      res = res + sentences\n",
    "  return res\n",
    "\n",
    "def pp(t):\n",
    "  return str(t.flatten()).replace('\\n', ' ').replace('   ', ' ')\n",
    "\n",
    "def text(t):\n",
    "  return ' '.join(t.leaves())\n",
    "\n",
    "def conjoin(a, b, left):\n",
    "  if left:\n",
    "    return Tree.fromstring(b.label() + ' (' + text(a) + ' ' + text(b) + ')')\n",
    "  else:\n",
    "    return Tree.fromstring(a.label() + ' (' + text(a) + ' ' + text(b) + ')')\n",
    "  \n",
    "def lowest_pos(t):\n",
    "  return [s for s in t.subtrees(lambda t: t.height() == 2)]\n",
    "\n",
    "def lowest_leaves(t):\n",
    "  return [t[s] for s in t.subtrees(lambda t: t.height() == 2)]\n",
    "#returns if corrective, anchored (verb negated)\n",
    "def find_s(t, pos0):\n",
    "  sentential_tags = {'S', 'SBAR', 'SINV', 'SBARQ', 'SQ'}\n",
    "  negators = {'not', 'n\\'t'}\n",
    "  verb_tags = {'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
    "  tensed = {'have', 'do', 'will', 'would', 'can', 'could', 'may', 'might', 'should', 'ought', 'must', 'shall', 'be'}\n",
    "\n",
    "  try:\n",
    "    while t[pos].label() not in sentential_tags:\n",
    "      pos = pos[:-1]\n",
    "  except:\n",
    "    return (False, False)\n",
    "  poses = lowest_pos(t)\n",
    "  i = None\n",
    "  for k in range(poses.index(pos0)):\n",
    "    if text(t[poses[k]]) in negators:\n",
    "      i = k\n",
    "      break\n",
    "  if i == None:\n",
    "    return (False, False)\n",
    "  try:\n",
    "    temp = wln().lemmatize(text(t[poses[i-1]]),pos='v')\n",
    "    if t[poses[i-1]].label() in verb_tags and temp in tensed and temp != 'VB':\n",
    "      return (True, True)\n",
    "    return (True, False)\n",
    "  except:\n",
    "    return (False, False)\n",
    "\n",
    "def funnel(tag):\n",
    "  if tag in verb_tags:\n",
    "    return \"V\"\n",
    "  elif tag in adverb_tags:\n",
    "    return 'R'\n",
    "  elif tag in adj_tags:\n",
    "    return 'J'\n",
    "  elif tag in noun_tags:\n",
    "    return 'N'\n",
    "  elif tag in sentential_tags:\n",
    "    return 'S'\n",
    "  else:\n",
    "    return 'X'\n",
    "\n",
    "\n",
    "def sent_apply(r):\n",
    "  s0 = Sentence(r)\n",
    "  tagger.predict(s0)\n",
    "  sentiment = s0.labels[0].value\n",
    "  score = s0.labels[0].score\n",
    "  if sentiment == 'NEGATIVE':\n",
    "    return -1 * score\n",
    "  return score\n",
    "\n",
    "def cosine_apply(p, q):\n",
    "  if ' ' in p or ' ' in q:\n",
    "    return np.NaN\n",
    "  return model.wv.similarity(p,q)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"VBD\" in verb_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverweizel/.pyenv/versions/3.12.7/lib/python3.12/site-packages/benepar/parse_chart.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "  1%|‚ñè         | 3.57M/253M [04:27<5:11:29, 14.0kB/s]\n",
      "/Users/oliverweizel/.pyenv/versions/3.12.7/lib/python3.12/site-packages/torch/distributions/distribution.py:56: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "parser = benepar.Parser(\"benepar_en3\")\n",
    "ans = valid_sentences('COCA/text_academic_rpe/w_acad_1990.txt', english_processor, english_indicator)[:100]\n",
    "df = pd.DataFrame({\"sentence\":ans})\n",
    "tree_gen = parser.parse_sents([benepar.InputSentence(s) for s in ans])\n",
    "trees = []\n",
    "for t in tree_gen:\n",
    "  trees.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TOP                                   \n",
      "          |                                     \n",
      "          S                                    \n",
      "  ________|________                             \n",
      " |                 VP                          \n",
      " |             ____|________________________    \n",
      " |            VP                        |   |  \n",
      " |    ________|________                 |   |   \n",
      " |   |    |            VP               |   |  \n",
      " |   |    |    ________|___             |   |   \n",
      " |   |    |   |            PP           |   |  \n",
      " |   |    |   |     _______|___         |   |   \n",
      " NP  |    |   |    |           NP       |   VP \n",
      " |   |    |   |    |        ___|___     |   |   \n",
      "PRP VBZ   RB  VB   IN      DT      NN   CC VBZ \n",
      " |   |    |   |    |       |       |    |   |   \n",
      " He does not walk  to     the     pool but runs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"He does not walk to the pool but runs\"\n",
    "bs = parser.parse_sents(txt1)\n",
    "ok = []\n",
    "for o in bs:\n",
    "  o.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentential_tags = {'S', 'SBAR', 'SINV', 'SBARQ', 'SQ'}\n",
    "negators = {'not', 'n\\'t'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| t: Tree('TOP', [Tree('SBAR', [Tree('WHNP', [Tree('-LRB-', ['-LRB-']), Tree('S', [Tree('SBAR', [Tree('WHNP', [Tree('WP', ['what'])]), Tree('S', [Tree('VP', [Tree('VBD', ['brought']), Tree('NP', [Tree('PRP', ['him'])]), Tree('NP', [Tree('NN', ['shame'])])])])]), Tree('VP', [Tree('VBD', ['was']), Tree('RB', ['not']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['pain']), Tree('CC', ['and']), Tree('NN', ['anger'])]), Tree('CC', ['but']), Tree('NP', [Tree('NP', [Tree('PRP$', ['his']), Tree('NN', ['trading'])]), Tree('PP', [Tree('IN', ['on']), Tree('NP', [Tree('PRP', ['them'])])]), Tree('PP', [Tree('IN', ['for']), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['what'])]), Tree('S', [Tree('VP', [Tree('VBD', ['turned']), Tree('PRT', [Tree('RP', ['out'])]), Tree('S', [Tree('VP', [Tree('TO', ['to']), Tree('VP', [Tree('VB', ['be']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['mere']), Tree('CD', ['twenty']), Tree('NNS', ['bucks'])])])])])])])])])])])])])])])])\n"
     ]
    }
   ],
   "source": [
    "parent_tree = []\n",
    "parent_text = []\n",
    "parent_type = []\n",
    "left_tree = []\n",
    "left_text = []\n",
    "left_type = []\n",
    "cc_tree = []\n",
    "cc_text = []\n",
    "right_tree = []\n",
    "right_text = []\n",
    "right_type = []\n",
    "corrective = []\n",
    "anchored = []\n",
    "clausal_coord = []\n",
    "base_text = []\n",
    "\n",
    "\n",
    "for t in trees:\n",
    "  #ic('full: '+ pp(t))\n",
    "  if 'the pain and anger' in text(t):\n",
    "    ic(t)\n",
    "  for tp in t.treepositions():\n",
    "    if isinstance(t[tp], str):\n",
    "      continue\n",
    "    if t[tp].label() == 'CC':\n",
    "      parent = t[tp[:-1]]\n",
    "      relative_tp = tp[-1]\n",
    "      # ic(relative_tp)\n",
    "      # ic(parent.treepositions())\n",
    "      if relative_tp + 1 >= len(parent) or relative_tp - 1 <= -1:\n",
    "        #print(\"bad conjunction\")\n",
    "        #ic(pp(parent))\n",
    "        continue\n",
    "      # try:\n",
    "      #   _ = parent[relative_tp + 1]\n",
    "      #   _ = parent[relative_tp - 1]\n",
    "      # except:\n",
    "      #   print(\"bad conjunction\")\n",
    "      #   ic(pp(parent))\n",
    "      #   continue\n",
    "      #ic('parent: ' + pp(parent))\n",
    "      left = parent[relative_tp - 1]\n",
    "      #ic('left: ' + pp(left))\n",
    "      conj = parent[relative_tp]\n",
    "      #ic('conj: ' + pp(conj))\n",
    "      #ic('conjunction: ' + ' '.join(conj.leaves()))\n",
    "      right = parent[relative_tp + 1]\n",
    "      #ic('right: ' + pp(right))\n",
    "      cc = False\n",
    "      if left.label() in sentential_tags and right.label() in sentential_tags:\n",
    "        cc = True\n",
    "      \n",
    "      if 'only' in left.leaves() or 'just' in left.leaves():\n",
    "        continue\n",
    "      if text(left) == 'but':\n",
    "        continue\n",
    "      if left.label() == 'CC':\n",
    "        continue\n",
    "      # try:\n",
    "      #   farleft = parent[relative_tp - 2]\n",
    "      #   if text(farleft) == 'not' and text(conj) == 'but':\n",
    "      #     corrective.append(True)\n",
    "      #     left = conjoin(farleft, left, True)\n",
    "      #   else:\n",
    "      #     corrective.append(False)\n",
    "      # except:\n",
    "      #   corrective.append(False)\n",
    "      corr, anch = find_s(t, tp)\n",
    "      # do the rest of this oliver!!!\n",
    "      try:\n",
    "        farright = parent[relative_tp + 2]\n",
    "        if right.label() == 'ADVP' and farright.label() != 'ADVP':\n",
    "          right = conjoin(right, farright, False)\n",
    "      except:\n",
    "        pass\n",
    "      \n",
    "\n",
    "      parent_tree.append(parent)\n",
    "      parent_text.append(text(parent))\n",
    "      parent_type.append(parent.label())\n",
    "      left_tree.append(left)\n",
    "      left_text.append(text(left))\n",
    "      left_type.append(left.label())\n",
    "      right_tree.append(right)\n",
    "      right_text.append(text(right))\n",
    "      right_type.append(right.label())\n",
    "      cc_tree.append(conj)\n",
    "      cc_text.append(text(conj))\n",
    "      corrective.append(corr)\n",
    "      anchored.append(anch)\n",
    "      clausal_coord.append(cc)\n",
    "      base_text.append(text(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_text\n",
      "and    34\n",
      "but     8\n",
      "or      2\n",
      "nor     1\n",
      "Name: count, dtype: int64\n",
      "Power_divergenceResult(statistic=np.float64(63.888888888888886), pvalue=np.float64(8.669650361275029e-14))\n",
      "                                         parent_tree  \\\n",
      "0  [[[(PRP i)], [(MD can), (VP\\n  (VB see)\\n  (AD...   \n",
      "1  [[[came], [(RB right), (RB out)]], [and], [[as...   \n",
      "2  [[[(PRP$ our), (NN color)], [(VBD had), (VP (V...   \n",
      "3  [[[smiled], [(IN at), (NP (DT the) (NN man))],...   \n",
      "4  [[[(RB certainly)], [(PRP i)], [(RB also)], [(...   \n",
      "\n",
      "                                         parent_text parent_type  \\\n",
      "0  i can see now that possibly she was but then a...           S   \n",
      "1  came right out and asked me to feel sorry for ...          VP   \n",
      "2  our color had brought this man anguish and if ...           S   \n",
      "3  smiled at the man to let him know he should n'...          VP   \n",
      "4  certainly i also judge him a coward but in tha...           S   \n",
      "\n",
      "                                           left_tree  \\\n",
      "0  [[[i]], [[can], [(VB see), (ADVP (RB now)), (S...   \n",
      "1                         [[came], [[right], [out]]]   \n",
      "2  [[[our], [color]], [[had], [(VBN brought), (NP...   \n",
      "3  [[smiled], [[at], [(DT the), (NN man)]], [[(TO...   \n",
      "4  [[[certainly]], [[i]], [[also]], [[judge], [(P...   \n",
      "\n",
      "                                           left_text left_type  \\\n",
      "0                i can see now that possibly she was         S   \n",
      "1                                     came right out        VP   \n",
      "2             our color had brought this man anguish         S   \n",
      "3  smiled at the man to let him know he should n'...        VP   \n",
      "4                certainly i also judge him a coward         S   \n",
      "\n",
      "                                          right_tree  \\\n",
      "0  [[[then]], [[(DT all)], [(S (NP (PRP i)) (VP (...   \n",
      "1  [[asked], [[me]], [[(TO to), (VP (VB feel) (AD...   \n",
      "2  [[[if], [(NP (NP (DT a) (NN part)) (PP (IN of)...   \n",
      "3                                           [[then]]   \n",
      "4  [[[in], [(DT that), (NN era)]], [[his], [cowar...   \n",
      "\n",
      "                                          right_text right_type cc_tree  ...  \\\n",
      "0  then all i saw was her irritability at having ...          S   [but]  ...   \n",
      "1                    asked me to feel sorry for them         VP   [and]  ...   \n",
      "2  if a part of that anguish was guilt it was not...          S   [and]  ...   \n",
      "3                                               then       ADVP   [and]  ...   \n",
      "4  in that era his cowardice was something i had ...          S   [but]  ...   \n",
      "\n",
      "  parent_funnel left_funnel right_funnel same_type same_funnel  clausal_coord  \\\n",
      "0             S           S            S      True        True           True   \n",
      "1             X           X            X      True        True          False   \n",
      "2             S           S            S      True        True           True   \n",
      "3             X           X            X     False        True          False   \n",
      "4             S           S            S      True        True           True   \n",
      "\n",
      "   corrective  left_sent  base_sent  right_sent  \n",
      "0       False   0.824752  -0.999281   -0.999338  \n",
      "1       False  -0.692926  -0.998004   -0.999035  \n",
      "2       False  -0.999757  -0.991380   -0.988480  \n",
      "3       False   0.997470   0.988396    0.812864  \n",
      "4       False  -0.999138  -0.928012   -0.927740  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'parent_tree':parent_tree, 'parent_text':parent_text, 'parent_type':parent_type, 'left_tree':left_tree, 'left_text':left_text, 'left_type':left_type, 'right_tree':right_tree, 'right_text':right_text, 'right_type':right_type, 'cc_tree':cc_tree, 'cc_text':cc_text})\n",
    "df['base_text'] = base_text\n",
    "df['parent_funnel'] = df.apply(lambda row: funnel(row['parent_type']), axis=1)\n",
    "df['left_funnel'] = df.apply(lambda row: funnel(row['left_type']), axis=1)\n",
    "df['right_funnel'] = df.apply(lambda row: funnel(row['right_type']), axis=1)\n",
    "df['same_type'] = df.apply(lambda row: row['left_type'] == row['right_type'], axis=1)\n",
    "df['same_funnel'] = df.apply(lambda row: row['left_funnel'] == row['right_funnel'], axis=1)\n",
    "df['clausal_coord'] = clausal_coord\n",
    "df['corrective'] = corrective\n",
    "df['left_sent'] = df.apply(lambda row: sent_apply(row['left_text']), axis=1)\n",
    "df['base_sent'] = df.apply(lambda row: sent_apply(row['base_text']), axis=1)\n",
    "df['right_sent'] = df.apply(lambda row: sent_apply(row['right_text']), axis=1)\n",
    "print(df['cc_text'].value_counts())\n",
    "res = chisquare(df['cc_text'].value_counts())\n",
    "print(res)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
